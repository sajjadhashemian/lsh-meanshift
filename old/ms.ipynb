{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'faiss_env (Python 3.11.11)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n faiss_env ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy\n",
    "import math\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hash_buckets(X, epsilon, t=None):\n",
    "    \"\"\"\n",
    "    Compute LSH hashes and group vectors into hash buckets.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): Input data of shape (n_samples, n_features).\n",
    "        epsilon (float): Hash grid size parameter.\n",
    "\n",
    "    Returns:\n",
    "        dict: Hash buckets with bucket keys and vector indices as values.\n",
    "    \"\"\"\n",
    "    if(t==None):\n",
    "        t=int(X.shape[0]**0.5)\n",
    "    \n",
    "    buckets = defaultdict(list)\n",
    "    hashes = defaultdict(list)\n",
    "    for _ in range(t):\n",
    "        eta = np.random.uniform(0, 2 * epsilon, X.shape[1])\n",
    "        _hashes = np.floor((X + eta) / (2 * epsilon))\n",
    "        for i, h in enumerate(_hashes):\n",
    "            _h=tuple(np.append(h, eta))\n",
    "            hashes[i].append(_h)\n",
    "            buckets[_h].append(i)\n",
    "    \n",
    "    return buckets, hashes\n",
    "\n",
    "def create_bucket_graph(buckets, hashes):\n",
    "    \"\"\"\n",
    "    Create a directed graph on hash buckets.\n",
    "\n",
    "    Args:\n",
    "        buckets (dict): Hash buckets with vector indices.\n",
    "\n",
    "    Returns:\n",
    "        dict: Directed adjacency list where edges are weighted by bucket size.\n",
    "    \"\"\"\n",
    "    graph = defaultdict(dict)\n",
    "    for u in hashes.keys():\n",
    "        for bucket_key in hashes[u]:\n",
    "            for other_bucket_key in hashes[u]:\n",
    "                if bucket_key == other_bucket_key:\n",
    "                    continue\n",
    "                if len(buckets[bucket_key]) > len(buckets[other_bucket_key]):\n",
    "                    weight = len(buckets[bucket_key])\n",
    "                    if other_bucket_key not in graph[bucket_key]:\n",
    "                        graph[bucket_key][other_bucket_key] = weight\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_graph(graph):\n",
    "    \"\"\"\n",
    "    Prune the graph to retain only maximum weight outgoing edges.\n",
    "\n",
    "    Args:\n",
    "        graph (dict): Directed graph adjacency list.\n",
    "\n",
    "    Returns:\n",
    "        dict: Pruned undirected adjacency list.\n",
    "    \"\"\"\n",
    "    pruned_graph = defaultdict(set)\n",
    "\n",
    "    for u in graph:\n",
    "        max_weight_edge = max(graph[u].items(), key=lambda x: x[1])[0]\n",
    "        pruned_graph[u].add(max_weight_edge)\n",
    "        pruned_graph[max_weight_edge].add(u)\n",
    "    \n",
    "    return pruned_graph\n",
    "\n",
    "def get_connected_components(graph):\n",
    "    \"\"\"\n",
    "    Perform DFS to find connected components in an undirected graph.\n",
    "\n",
    "    Args:\n",
    "        graph (dict): Undirected adjacency list.\n",
    "\n",
    "    Returns:\n",
    "        list: List of connected components as sets of bucket keys.\n",
    "    \"\"\"\n",
    "    visited = set()\n",
    "    connected_components = []\n",
    "\n",
    "    def dfs(node, component):\n",
    "        visited.add(node)\n",
    "        component.add(node)\n",
    "        for neighbor in graph[node]:\n",
    "            if neighbor not in visited:\n",
    "                dfs(neighbor, component)\n",
    "\n",
    "    for node in graph:\n",
    "        if node not in visited:\n",
    "            component = set()\n",
    "            dfs(node, component)\n",
    "            connected_components.append(component)\n",
    "    \n",
    "    return connected_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_shift_clustering(X, epsilon):\n",
    "    \"\"\"\n",
    "    Perform Quick-Shift clustering.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): Input data of shape (n_samples, n_features).\n",
    "        epsilon (float): Hash grid size parameter.\n",
    "\n",
    "    Returns:\n",
    "        list: Clusters as connected components of vectors.\n",
    "    \"\"\"\n",
    "    buckets, hashes = compute_hash_buckets(X, epsilon)\n",
    "    graph = create_bucket_graph(buckets, hashes)\n",
    "    pruned_graph = prune_graph(graph)\n",
    "    connected_components = get_connected_components(pruned_graph)\n",
    "\n",
    "    clusters = []\n",
    "    for component in connected_components:\n",
    "        cluster = []\n",
    "        for bucket_key in component:\n",
    "            cluster.extend(buckets[bucket_key])\n",
    "        clusters.append(cluster)\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_quick_shift():\n",
    "    X, y = make_blobs(n_samples=300, centers=2, cluster_std=0.60, random_state=0)\n",
    "    \n",
    "    # Parameters\n",
    "    epsilon = 3.0\n",
    "\n",
    "    # Perform clustering\n",
    "    clusters = quick_shift_clustering(X, epsilon)\n",
    "    \n",
    "    # plt.scatter(X[:, 0], X[:, 1], c='gray', alpha=0.5, label=\"Data Points\")\n",
    "    for cluster_idx, cluster in enumerate(clusters):\n",
    "        print(len(cluster))\n",
    "        cluster_points = X[cluster]\n",
    "        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f\"Cluster {cluster_idx}\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Quick-Shift Clustering with LSH\")\n",
    "    plt.show()\n",
    "\n",
    "test_quick_shift()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
